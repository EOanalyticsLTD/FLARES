{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import zipfile\n",
    "import os, sys, glob\n",
    "from pathlib import Path\n",
    "\n",
    "# adds the package path to the Python path to make sure all the local imports work fine \n",
    "if os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))) not in sys.path:\n",
    "    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))))\n",
    "    \n",
    "from wp4.constants import S3_ACCESS_KEY, S3_SECRET_KEY, S3_ENDPOINT, DATA_DIR_GFAS, DATA_DIR_ERA5, DATA_DIR_CAMS_AN, DATA_DIR_CAMS_RE, DATA_DIR_LC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d6018",
   "metadata": {},
   "source": [
    "### Initiate S3 client & check buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=S3_ACCESS_KEY,\n",
    "    aws_secret_access_key=S3_SECRET_KEY,\n",
    "    endpoint_url=S3_ENDPOINT\n",
    ")\n",
    "\n",
    "existing_bucket_names = [bucket['Name'] for bucket in s3_client.list_buckets()['Buckets']]\n",
    "print(f'Current buckets: {\", \".join(existing_bucket_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_BUCKET = \"cams-analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae66f82",
   "metadata": {},
   "source": [
    "### Checking Bucket Names and creating target bucket if is does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664d6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if bucket exists and if not create new bucket\n",
    "existing_bucket_names = [bucket['Name'] for bucket in s3_client.list_buckets()['Buckets']]\n",
    "\n",
    "if not TARGET_BUCKET in existing_bucket_names:\n",
    "    print(f'Creating S3 bucket: {TARGET_BUCKET}')\n",
    "    try:\n",
    "        s3_client.create_bucket(Bucket=TARGET_BUCKET)\n",
    "    except:\n",
    "        raise\n",
    "    else:\n",
    "        print(f'Bucket {TARGET_BUCKET} created')\n",
    "        existing_bucket_names = [bucket['Name'] for bucket in s3_client.list_buckets()['Buckets']]\n",
    "        print(f'Current buckets: {\", \".join(existing_bucket_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa318efe",
   "metadata": {},
   "source": [
    "### Zip & upload single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('path')  # Directory where the file is located\n",
    "FILE_NAME = '' # name of the file\n",
    "FILE_TYPE = '.nc'  # file extension .nc .tif etc\n",
    "OVERWRITE_FILE = False  # overwrite the file if already present in the bucket\n",
    "\n",
    "os.chdir(DATA_DIR)\n",
    "\n",
    "file_name = f'{FILE_NAME}{FILE_TYPE}'\n",
    "out_zip_file = f'{FILE_NAME}.zip'\n",
    "out_zip_file_loc = DATA_DIR.joinpath(f'{FILE_NAME}.zip')\n",
    "  \n",
    "# create zip folder with the file we want to compress \n",
    "zipfile.ZipFile(out_zip_file, mode='w').write(\n",
    "    file_name,\n",
    "    compress_type=zipfile.ZIP_DEFLATED\n",
    ")\n",
    "\n",
    "# list the current contents in the bucket\n",
    "bucket_contents = s3_client.list_objects_v2(Bucket=TARGET_BUCKET)\n",
    "\n",
    "if \"Contents\" in bucket_contents.keys():\n",
    "    bucket_content_names  = [f['Key'] for f in bucket_contents[\"Contents\"]]\n",
    "else:\n",
    "    bucket_content_names  = []\n",
    "    \n",
    "\n",
    "if out_zip_file in bucket_content_names and not OVERWRITE_FILE:\n",
    "    # if there is already a file with the same name  in the bucket and we do not want to overwrite\n",
    "    print(f'{file_name} already present in bucket')\n",
    "elif (out_zip_file in bucket_content_names) and OVERWRITE_FILE:\n",
    "    # if there is already a file with the same name in the bucket and we do want to overwrite\n",
    "    print(f'{out_zip_file} already present in bucket. Will overwrite this file.')\n",
    "    print(f'Starting upload of {out_zip_file} to bucket: {TARGET_BUCKET}')\n",
    "    try:\n",
    "        s3_client.upload_file(out_zip_file_loc.as_posix(), TARGET_BUCKET, out_zip_file)\n",
    "    except:\n",
    "        raise\n",
    "    else:\n",
    "        print(f'Upload of {out_zip_file} to bucket: {TARGET_BUCKET} completed')\n",
    "else:\n",
    "    # if there is no file with the same name  in the bucket\n",
    "    print(f'Starting upload of {out_zip_file} to bucket: {TARGET_BUCKET}')\n",
    "    try:\n",
    "        s3_client.upload_file(out_zip_file_loc.as_posix(), TARGET_BUCKET, out_zip_file)\n",
    "    except:\n",
    "        raise\n",
    "    else:\n",
    "        print(f'Upload of {out_zip_file} to bucket: {TARGET_BUCKET} completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc544d6",
   "metadata": {},
   "source": [
    "### Zip & upload multiple files of single file type within folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r\"path_to_datadir\"  # Directory where the file is located\n",
    "FOLDER_NAME = 'folder_name.zip' # name of the zip file uploaded to the bucket\n",
    "FILE_TYPE = '.nc'  # file extension of the files in the folder to zip \n",
    "OVERWRITE_FILE = False  # overwrite the file if already present in the bucket\n",
    "\n",
    "os.chdir(DATA_DIR)\n",
    "\n",
    "# compress and store all files in a zip folder\n",
    "with zipfile.ZipFile(FOLDER_NAME, 'w') as _zip:\n",
    "    for file_name in glob.glob(f'{DATA_DIR}/*{FILE_TYPE}'):\n",
    "        out_zip_file = Path(file_name).name\n",
    "        _zip.write(out_zip_file, compress_type=zipfile.ZIP_DEFLATED)\n",
    "\n",
    "# list current bucket contents\n",
    "bucket_contents = s3_client.list_objects_v2(Bucket=TARGET_BUCKET)\n",
    "\n",
    "if \"Contents\" in bucket_contents.keys():\n",
    "    bucket_content_names  = [f['Key'] for f in bucket_contents[\"Contents\"]]\n",
    "else:\n",
    "    bucket_content_names  = []\n",
    "\n",
    "# uploading    \n",
    "if FOLDER_NAME in bucket_content_names and not OVERWRITE_FILE:\n",
    "    # if there is already a file with the same name  in the bucket and we do not want to overwrite\n",
    "    print(f'{FOLDER_NAME} already present in bucket')\n",
    "elif FOLDER_NAME in bucket_content_names and OVERWRITE_FILE:\n",
    "    # if there is already a file with the same name in the bucket and we do want to overwrite\n",
    "    try:\n",
    "        print(f'File {FOLDER_NAME} already present in bucket. Will overwrite this file.')\n",
    "        print(f'Starting upload of {FOLDER_NAME} to bucket: {TARGET_BUCKET}')\n",
    "        s3_client.upload_file(Path(DATA_DIR).joinpath(FOLDER_NAME).as_posix(), TARGET_BUCKET, FOLDER_NAME)\n",
    "    except:\n",
    "        raise\n",
    "    else:\n",
    "        print(f'Upload of {FOLDER_NAME} to bucket: {TARGET_BUCKET} completed')\n",
    "else:\n",
    "    # if there is no file with the same name  in the bucket\n",
    "    try:\n",
    "        print(f'Starting upload of {FOLDER_NAME} to bucket: {TARGET_BUCKET}')\n",
    "        s3_client.upload_file(Path(DATA_DIR).joinpath(FOLDER_NAME).as_posix(), TARGET_BUCKET, FOLDER_NAME)\n",
    "    except:\n",
    "        raise\n",
    "    else:\n",
    "        print(f'Upload of {FOLDER_NAME} to bucket: {TARGET_BUCKET} completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0fa510",
   "metadata": {},
   "source": [
    "### Check the contents of the target bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df089e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_contents = s3_client.list_objects_v2(Bucket=TARGET_BUCKET)\n",
    "\n",
    "if \"Contents\" in bucket_contents.keys():\n",
    "    bucket_content_names  = [f['Key'] for f in bucket_contents[\"Contents\"]]\n",
    "    print(f'Current contents of bucket {TARGET_BUCKET}: {\", \".join(bucket_content_names)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01cf51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove zip archives\n",
    "\n",
    "for file in glob.glob(f'{DATA_DIR}*.zip'):\n",
    "    os.remove(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
